import requests  # ë„¤ì´ë²„ API ë° í…”ë ˆê·¸ë¨ ì„œë²„ì™€ í†µì‹ í•˜ê¸° ìœ„í•´ ì‚¬ìš©í•©ë‹ˆë‹¤.
import os        # ì‹œìŠ¤í…œ í™˜ê²½ ë³€ìˆ˜(Secrets) ë° íŒŒì¼ ê²½ë¡œ ì²˜ë¦¬ë¥¼ ìœ„í•´ ì‚¬ìš©í•©ë‹ˆë‹¤.
from datetime import datetime, timedelta  # ë‚ ì§œì™€ ì‹œê°„ ê³„ì‚°ì„ ìœ„í•´ ì‚¬ìš©í•©ë‹ˆë‹¤.
import pytz      # í•œêµ­ ì‹œê°„ëŒ€(KST)ë¥¼ ì •í™•í•˜ê²Œ ë§ì¶”ê¸° ìœ„í•´ ì‚¬ìš©í•©ë‹ˆë‹¤.

# 1. ê¹ƒí—ˆë¸Œ ì‹œí¬ë¦¿(Secrets)ì— ì €ì¥ëœ ë³´ì•ˆ í‚¤ë“¤ì„ ë¶ˆëŸ¬ì˜µë‹ˆë‹¤.
NAVER_ID = os.environ.get('NAVER_CLIENT_ID')
NAVER_SECRET = os.environ.get('NAVER_CLIENT_SECRET')
TG_TOKEN = os.environ.get('TELEGRAM_TOKEN')
CHAT_ID = os.environ.get('CHAT_ID')

# [í•¨ìˆ˜] keywords.txt íŒŒì¼ì—ì„œ ê²€ìƒ‰ì–´ ëª©ë¡ì„ ì½ì–´ì˜µë‹ˆë‹¤.
def load_keywords():
    filename = "keywords.txt"
    if os.path.exists(filename):
        with open(filename, "r", encoding="utf-8") as f:
            # ì–‘ ë ê³µë°±ì„ ì§€ìš°ê³  ë‚´ìš©ì´ ìˆëŠ” ì¤„ë§Œ ë¦¬ìŠ¤íŠ¸ë¡œ ë°˜í™˜í•©ë‹ˆë‹¤.
            return [line.strip() for line in f.read().splitlines() if line.strip()]
    return []

# [í•¨ìˆ˜] ë„¤ì´ë²„ ë‰´ìŠ¤ APIì— ì ‘ì†í•˜ì—¬ ê¸°ì‚¬ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤.
def get_news(keyword, sort_type):
    query = f'"{keyword}"'  # ì •í™•í•œ ê²€ìƒ‰ì„ ìœ„í•´ í‚¤ì›Œë“œ ì–‘ì˜†ì— í°ë”°ì˜´í‘œë¥¼ ë¶™ì…ë‹ˆë‹¤.
    # ê°ê° 30ê°œì”© ìœ ì‚¬ë„ìˆœ(sim) ë˜ëŠ” ìµœì‹ ìˆœ(date)ìœ¼ë¡œ ìš”ì²­í•©ë‹ˆë‹¤.
    url = f"https://openapi.naver.com/v1/search/news.json?query={query}&display=30&sort={sort_type}"
    headers = {"X-Naver-Client-Id": NAVER_ID, "X-Naver-Client-Secret": NAVER_SECRET}
    try:
        res = requests.get(url, headers=headers)
        return res.json().get('items', [])
    except:
        return []

# [í•¨ìˆ˜] í…”ë ˆê·¸ë¨ìœ¼ë¡œ ë©”ì‹œì§€ë¥¼ ë³´ëƒ…ë‹ˆë‹¤.
def send_tg(text):
    if not text.strip(): return
    url = f"https://api.telegram.org/bot{TG_TOKEN}/sendMessage"
    # HTML í˜•ì‹ì„ ì§€ì›í•˜ê³  ë§í¬ ë¯¸ë¦¬ë³´ê¸°ë¥¼ ë¹„í™œì„±í™”í•˜ì—¬ ê¹”ë”í•˜ê²Œ ë³´ëƒ…ë‹ˆë‹¤.
    payload = {"chat_id": CHAT_ID, "text": text, "parse_mode": "HTML", "disable_web_page_preview": True}
    requests.post(url, json=payload)

# [í•¨ìˆ˜] ë‰´ìŠ¤ ì œëª©ì˜ HTML íƒœê·¸ë¥¼ ì§€ì›ë‹ˆë‹¤.
def clean_title(title):
    return title.replace("<b>", "").replace("</b>", "").replace("&quot;", '"').replace("&amp;", "&").replace("&lt;", "<").replace("&gt;", ">").strip()

# [í•¨ìˆ˜] ë‚ ì§œë¥¼ í•œêµ­ì¸ì´ ì½ê¸° í¸í•œ í¬ë§·ìœ¼ë¡œ ë°”ê¿‰ë‹ˆë‹¤.
def format_date_kor(date_str):
    try:
        weekday_map = {"Mon": "ì›”", "Tue": "í™”", "Wed": "ìˆ˜", "Thu": "ëª©", "Fri": "ê¸ˆ", "Sat": "í† ", "Sun": "ì¼"}
        dt_obj = datetime.strptime(date_str, '%a, %d %b %Y %H:%M:%S +0900')
        kor_w = weekday_map.get(dt_obj.strftime('%a'), dt_obj.strftime('%a'))
        return dt_obj.strftime(f'%Y.%m.%d.({kor_w}) %H:%M')
    except:
        return date_str

# --- [ë©”ì¸ ì‹¤í–‰ ë¡œì§] ---

# í•œêµ­ ì‹œê°„ëŒ€ ì„¤ì •
korea_tz = pytz.timezone('Asia/Seoul')
now_korea = datetime.now(korea_tz)

# ìƒˆë²½ ì‹œê°„(0~5ì‹œ)ì—ëŠ” ì•Œë¦¼ì„ ë³´ë‚´ì§€ ì•Šê³  ì¢…ë£Œí•©ë‹ˆë‹¤.
if 0 <= now_korea.hour < 6:
    exit()

# ê¸°ì¤€ ì‹œê°„ ì„¤ì •: í˜„ì¬ë¡œë¶€í„° ì´í‹€(48ì‹œê°„) ì „
two_days_ago = now_korea - timedelta(days=2)

# 1. ê¸°ë¡ íŒŒì¼(sent_links.txt) ë¡œë“œ ë° ì´í‹€ ì§€ë‚œ ë°ì´í„° í•„í„°ë§
DB_FILE = "sent_links.txt"
valid_records = {}  # { 'ê¸°ì‚¬ë§í¬': 'ì €ì¥ë‚ ì§œ' } í˜•íƒœë¡œ ì €ì¥í•©ë‹ˆë‹¤.
if os.path.exists(DB_FILE):
    with open(DB_FILE, "r", encoding="utf-8") as f:
        for line in f:
            if "|" in line:
                link, save_date_str = line.strip().split("|")
                try:
                    # ì €ì¥ëœ ë‚ ì§œë¥¼ ë¶„ì„í•˜ì—¬ ì´í‹€ ì´ë‚´ì˜ ê¸°ë¡ë§Œ ìœ ì§€í•©ë‹ˆë‹¤.
                    save_dt = datetime.strptime(save_date_str, '%Y%m%d%H%M')
                    save_dt = korea_tz.localize(save_dt)
                    if save_dt > two_days_ago:
                        valid_records[link] = save_date_str
                except:
                    continue

KEYWORDS = load_keywords()
all_collected_articles = []  # ìƒˆë¡œ ë°œê²¬ëœ ê¸°ì‚¬ë“¤ì„ ëª¨ìœ¼ëŠ” ë°”êµ¬ë‹ˆ
no_news_keywords = []        # ìƒˆë¡œìš´ ì†Œì‹ì´ ì—†ëŠ” í‚¤ì›Œë“œë“¤ì„ ëª¨ìœ¼ëŠ” ë°”êµ¬ë‹ˆ

# 2. í‚¤ì›Œë“œë³„ ìˆ˜ì§‘ ì‹œì‘
for kw in KEYWORDS:
    # ìœ ì‚¬ë„ìˆœê³¼ ìµœì‹ ìˆœ ê²°ê³¼ë¥¼ í•©ì¹©ë‹ˆë‹¤.
    raw_candidates = get_news(kw, "sim") + get_news(kw, "date")
    current_titles = []  # ë™ì¼ ì‹¤í–‰ ë‚´ ì¤‘ë³µ ê¸°ì‚¬ ë°©ì§€ìš©
    found_new_for_this_kw = False
    
    # ê²€ìƒ‰ ê²°ê³¼ê°€ ì•„ì˜ˆ ì—†ëŠ” ê²½ìš°
    if not raw_candidates:
        no_news_keywords.append(kw)
        continue

    for item in raw_candidates:
        link = item['link']
        title = clean_title(item['title'])
        pub_date_raw = item.get('pubDate', '')
        
        # [ê²€ì¦ 1] ê¸°ì‚¬ ë°œí–‰ì¼ì´ ì´í‹€ ì´ë‚´ì¸ì§€ í™•ì¸
        try:
            pub_dt = datetime.strptime(pub_date_raw, '%a, %d %b %Y %H:%M:%S +0900')
            pub_dt = korea_tz.localize(pub_dt)
            if pub_dt < two_days_ago:
                continue
        except:
            continue
            
        # [ê²€ì¦ 2] ì´ë¯¸ ë³´ë‚¸ ì ì´ ìˆëŠ” ë§í¬ì¸ì§€ í™•ì¸
        if link in valid_records:
            continue
            
        # [ê²€ì¦ 3] ì œëª© ì• 15ìê°€ ê²¹ì¹˜ë©´ ì¤‘ë³µ ê¸°ì‚¬ë¡œ ê°„ì£¼í•˜ì—¬ íŒ¨ìŠ¤
        prefix = title[:15]
        if any(prefix in t for t in current_titles):
            continue
        
        current_titles.append(title)
        found_new_for_this_kw = True
        
        # ì œëª©ì— í‚¤ì›Œë“œ í¬í•¨ ì‹œ ìš°ì„ ìˆœìœ„ ìƒí–¥ (0ì´ ë†’ìŒ)
        has_kw = 0 if kw.lower() in title.lower() else 1
        
        all_collected_articles.append({
            "kw": kw, "title": title, "link": link,
            "raw_date": pub_dt, "kor_date": format_date_kor(pub_date_raw),
            "priority": has_kw
        })
        # ìƒˆë¡œìš´ ê¸°ë¡ì„ ë©”ëª¨ë¦¬ì— ì €ì¥í•©ë‹ˆë‹¤.
        valid_records[link] = now_korea.strftime('%Y%m%d%H%M')

    # í•„í„°ë§ í›„ì—ë„ ë³´ë‚¼ ê¸°ì‚¬ê°€ ì—†ìœ¼ë©´ 'ìƒˆ ì†Œì‹ ì—†ìŒ' ë¦¬ìŠ¤íŠ¸ì— ì¶”ê°€
    if not found_new_for_this_kw:
        no_news_keywords.append(kw)

# 3. ìƒˆë¡œìš´ ê¸°ì‚¬ ë°œì†¡ ë¡œì§
if all_collected_articles:
    # ìš°ì„ ìˆœìœ„(í‚¤ì›Œë“œ í¬í•¨ ì—¬ë¶€)ì™€ ìµœì‹ ìˆœìœ¼ë¡œ ì •ë ¬í•©ë‹ˆë‹¤.
    all_collected_articles.sort(key=lambda x: (x['priority'], x['raw_date']), reverse=True)
    all_collected_articles.sort(key=lambda x: x['priority'])
    
    formatted_msgs = []
    for art in all_collected_articles:
        formatted_msgs.append(f"â€¢ <b>[{art['kw']}]</b> {art['title']}\n  ğŸ•’ {art['kor_date']}\n  <a href='{art['link']}'>ê¸°ì‚¬ë³´ê¸°</a>")

    # ê¸°ì‚¬ë¥¼ 20ê°œì”© ë¬¶ì–´ì„œ ë³´ëƒ…ë‹ˆë‹¤.
    for i in range(0, len(formatted_msgs), 20):
        chunk = formatted_msgs[i:i + 20]
        send_tg("<b>[ì‹¤ì‹œê°„ ë‰´ìŠ¤ ë¦¬í¬íŠ¸]</b>\n\n" + "\n\n".join(chunk))

# 4. ì†Œì‹ì´ ì—†ëŠ” í‚¤ì›Œë“œ ì•Œë¦¼ ë¡œì§ (ëª©ë¡ìœ¼ë¡œ ë¬¶ì–´ì„œ ë°œì†¡)
if no_news_keywords:
    # í‚¤ì›Œë“œê°€ ë„ˆë¬´ ë§ì„ ê²½ìš°ë¥¼ ëŒ€ë¹„í•´ 30ê°œì”© ëŠì–´ì„œ ë³´ëƒ…ë‹ˆë‹¤.
    for i in range(0, len(no_news_keywords), 30):
        chunk = no_news_keywords[i:i + 30]
        status_msg = "<b>[ì•Œë¦¼: ìƒˆë¡œìš´ ê¸°ì‚¬ ì—†ìŒ]</b>\n\n"
        status_msg += "\n".join([f"- {k}" for k in chunk])
        status_msg += "\n\nìœ„ í‚¤ì›Œë“œë“¤ì€ ìµœê·¼ 2ì¼ ë‚´ ìƒˆë¡œìš´ ê¸°ì‚¬ê°€ ì—†ìŠµë‹ˆë‹¤."
        send_tg(status_msg)

# 5. ìµœì¢… ìœ íš¨ ê¸°ë¡(ì´í‹€ ì´ë‚´)ì„ íŒŒì¼ì— ì €ì¥í•©ë‹ˆë‹¤.
with open(DB_FILE, "w", encoding="utf-8") as f:
    for link, date_str in valid_records.items():
        f.write(f"{link}|{date_str}\n")

print(f"ì‘ì—… ì™„ë£Œ: ì‹ ê·œ {len(all_collected_articles)}ê±´ ë°œì†¡ / ì†Œì‹ ì—†ìŒ {len(no_news_keywords)}ê±´ ë³´ê³ ")
