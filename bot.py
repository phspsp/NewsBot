import requests  # ë„¤ì´ë²„/í…”ë ˆê·¸ë¨ ì„œë²„ì™€ ë°ì´í„°ë¥¼ ì£¼ê³ ë°›ê¸° ìœ„í•œ ë„êµ¬ì…ë‹ˆë‹¤.
import os        # ì‹œìŠ¤í…œ í™˜ê²½ ë³€ìˆ˜ë‚˜ íŒŒì¼ ê²½ë¡œë¥¼ ë‹¤ë£¨ê¸° ìœ„í•´ ì‚¬ìš©í•©ë‹ˆë‹¤.
from datetime import datetime, timedelta  # í˜„ì¬ ì‹œê°„ ê³„ì‚° ë° ì¼ì£¼ì¼ ì „ ë‚ ì§œë¥¼ êµ¬í•˜ê¸° ìœ„í•´ ì‚¬ìš©í•©ë‹ˆë‹¤.
import pytz      # í•œêµ­ í‘œì¤€ì‹œ(KST)ë¥¼ ì •í™•í•˜ê²Œ ì„¤ì •í•˜ê¸° ìœ„í•´ ì‚¬ìš©í•©ë‹ˆë‹¤.

# 1. í™˜ê²½ ë³€ìˆ˜(GitHub Secrets)ì—ì„œ ë³´ì•ˆ í‚¤ ì •ë³´ë¥¼ ì•ˆì „í•˜ê²Œ ê°€ì ¸ì˜µë‹ˆë‹¤.
NAVER_ID = os.environ.get('NAVER_CLIENT_ID')      # ë„¤ì´ë²„ API ID
NAVER_SECRET = os.environ.get('NAVER_CLIENT_SECRET')  # ë„¤ì´ë²„ API ë¹„ë°€í‚¤
TG_TOKEN = os.environ.get('TELEGRAM_TOKEN')       # í…”ë ˆê·¸ë¨ ë´‡ í† í°
CHAT_ID = os.environ.get('CHAT_ID')               # ì•Œë¦¼ ë°›ì„ ì±„íŒ…ë°© ID

# [í•¨ìˆ˜] keywords.txt íŒŒì¼ì—ì„œ ê²€ìƒ‰ì–´ ëª©ë¡ì„ í•œ ì¤„ì”© ì½ì–´ì˜µë‹ˆë‹¤.
def load_keywords():
    filename = "keywords.txt"
    if os.path.exists(filename):  # íŒŒì¼ì´ ì‹¤ì œë¡œ ìˆì„ ë•Œë§Œ ì‹¤í–‰í•©ë‹ˆë‹¤.
        with open(filename, "r", encoding="utf-8") as f:
            # ì–‘ ë ê³µë°±ì„ ì§€ìš°ê³ , ë¹ˆ ì¤„ì´ ì•„ë‹Œ ê²ƒë“¤ë§Œ ë¦¬ìŠ¤íŠ¸ë¡œ ë§Œë“­ë‹ˆë‹¤.
            return [line.strip() for line in f.read().splitlines() if line.strip()]
    return ["ì‚¼ì„±ì „ì"]  # íŒŒì¼ì´ ì—†ìœ¼ë©´ ê¸°ë³¸ í‚¤ì›Œë“œë¡œ ê²€ìƒ‰í•©ë‹ˆë‹¤.

# [í•¨ìˆ˜] ë„¤ì´ë²„ ë‰´ìŠ¤ APIë¥¼ í˜¸ì¶œí•˜ì—¬ ê¸°ì‚¬ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤.
def get_news(keyword, sort_type):
    query = f'"{keyword}"'  # ì •í™•í•œ ê²€ìƒ‰ì„ ìœ„í•´ í‚¤ì›Œë“œì— í°ë”°ì˜´í‘œë¥¼ ë¶™ì…ë‹ˆë‹¤.
    # ê° í‚¤ì›Œë“œë‹¹ 30ê°œì”© ìš”ì²­í•©ë‹ˆë‹¤.
    url = f"https://openapi.naver.com/v1/search/news.json?query={query}&display=30&sort={sort_type}"
    headers = {"X-Naver-Client-Id": NAVER_ID, "X-Naver-Client-Secret": NAVER_SECRET}
    try:
        res = requests.get(url, headers=headers)
        return res.json().get('items', []) # ê²°ê³¼ë¬¼ ì¤‘ ê¸°ì‚¬ ë¦¬ìŠ¤íŠ¸(items)ë§Œ ë½‘ì•„ì˜µë‹ˆë‹¤.
    except:
        return []

# [í•¨ìˆ˜] í…”ë ˆê·¸ë¨ìœ¼ë¡œ ìµœì¢… ê²°ê³¼ ë©”ì‹œì§€ë¥¼ ë³´ëƒ…ë‹ˆë‹¤.
def send_tg(text):
    if not text.strip(): return # ë³´ë‚¼ ë‚´ìš©ì´ ì—†ìœ¼ë©´ ì•„ë¬´ê²ƒë„ ì•ˆ í•©ë‹ˆë‹¤.
    url = f"https://api.telegram.org/bot{TG_TOKEN}/sendMessage"
    # HTML í˜•ì‹ì„ í—ˆìš©í•˜ê³ , ë§í¬ ë¯¸ë¦¬ë³´ê¸° í™”ë©´ì€ êº¼ì„œ ê¹”ë”í•˜ê²Œ ë§Œë“­ë‹ˆë‹¤.
    payload = {"chat_id": CHAT_ID, "text": text, "parse_mode": "HTML", "disable_web_page_preview": True}
    requests.post(url, json=payload)

# [í•¨ìˆ˜] ê¸°ì‚¬ ì œëª©ì— í¬í•¨ëœ HTML íƒœê·¸ì™€ íŠ¹ìˆ˜ê¸°í˜¸ë¥¼ ê¹¨ë—í•˜ê²Œ ì •ë¦¬í•©ë‹ˆë‹¤.
def clean_title(title):
    return title.replace("<b>", "").replace("</b>", "").replace("&quot;", '"').replace("&amp;", "&").replace("&lt;", "<").replace("&gt;", ">").strip()

# [í•¨ìˆ˜] ì˜ì–´ ë‚ ì§œë¥¼ í•œêµ­ì–´ ìŠ¤íƒ€ì¼(2026.01.31.(í† ) 22:10)ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.
def format_date_kor(date_str):
    try:
        weekday_map = {"Mon": "ì›”", "Tue": "í™”", "Wed": "ìˆ˜", "Thu": "ëª©", "Fri": "ê¸ˆ", "Sat": "í† ", "Sun": "ì¼"}
        dt_obj = datetime.strptime(date_str, '%a, %d %b %Y %H:%M:%S +0900') # ì˜ì–´ ë‚ ì§œ ë¶„ì„
        kor_w = weekday_map.get(dt_obj.strftime('%a'), dt_obj.strftime('%a')) # ìš”ì¼ ë²ˆì—­
        return dt_obj.strftime(f'%Y.%m.%d.({kor_w}) %H:%M') # í•œêµ­ì‹ ì¬êµ¬ì„±
    except:
        return date_str # ë³€í™˜ ì‹¤íŒ¨ ì‹œ ì›ë³¸ì„ ê·¸ëŒ€ë¡œ ë‘¡ë‹ˆë‹¤.

# --- [ë©”ì¸ ì‹¤í–‰ ë¶€ë¶„] ---

# 1. í•œêµ­ ì‹œê°„ëŒ€ ì„¤ì • ë° ìƒˆë²½ ë°œì†¡ ê¸ˆì§€ ì²´í¬
korea_tz = pytz.timezone('Asia/Seoul')
now_korea = datetime.now(korea_tz)

if 0 <= now_korea.hour < 6: # ìƒˆë²½ 0ì‹œ ~ ì•„ì¹¨ 6ì‹œ ì‚¬ì´ë¼ë©´
    print(f"í˜„ì¬ {now_korea.hour}ì‹œì…ë‹ˆë‹¤. ìƒˆë²½ì—ëŠ” ì•Œë¦¼ì„ ë³´ë‚´ì§€ ì•ŠìŠµë‹ˆë‹¤.")
    exit() # í”„ë¡œê·¸ë¨ ì¢…ë£Œ

# 2. ì¼ì£¼ì¼ ì´ë‚´ì˜ ê¸°ì‚¬ë§Œ ìˆ˜ì§‘í•˜ê¸° ìœ„í•œ ê¸°ì¤€ ì‹œê°„ ê³„ì‚°
one_week_ago = now_korea - timedelta(days=7)

# 3. í‚¤ì›Œë“œ ë° ì¤‘ë³µ ë°©ì§€ DB(450ê°œ ì €ì¥ìš©) ë¶ˆëŸ¬ì˜¤ê¸°
KEYWORDS = load_keywords()
DB_FILE = "sent_links.txt"
sent_links = set()
if os.path.exists(DB_FILE):
    with open(DB_FILE, "r") as f:
        sent_links = set(f.read().splitlines())

all_collected_articles = [] # ëª¨ë“  í›„ë³´ ê¸°ì‚¬ë¥¼ ë‹´ì„ ë°”êµ¬ë‹ˆ

# 4. ê° í‚¤ì›Œë“œë³„ë¡œ ë‰´ìŠ¤ ìˆ˜ì§‘ ë° í•„í„°ë§ ì‹œì‘
for kw in KEYWORDS:
    # ìœ ì‚¬ë„ìˆœ 30ê°œì™€ ìµœì‹ ìˆœ 30ê°œë¥¼ ëª¨ë‘ í•©ì¹©ë‹ˆë‹¤.
    raw_candidates = get_news(kw, "sim") + get_news(kw, "date")
    current_titles = [] # ë„ë°° ë°©ì§€ìš© ì„ì‹œ ë¦¬ìŠ¤íŠ¸
    
    for item in raw_candidates:
        link = item['link']
        title = clean_title(item['title'])
        pub_date_raw = item.get('pubDate', '')
        
        # [ê²€ì¦ A] ì¼ì£¼ì¼ ì´ë‚´ ê¸°ì‚¬ì¸ì§€ í™•ì¸
        try:
            pub_dt = datetime.strptime(pub_date_raw, '%a, %d %b %Y %H:%M:%S +0900')
            pub_dt = korea_tz.localize(pub_dt)
            if pub_dt < one_week_ago: # ì¼ì£¼ì¼ì´ ë„˜ì—ˆë‹¤ë©´
                continue # ê±´ë„ˆëœë‹ˆë‹¤.
        except:
            pub_dt = now_korea
            
        # [ê²€ì¦ B] ì´ë¯¸ ë³´ëƒˆë˜ ë§í¬ì¸ì§€ í™•ì¸
        if link in sent_links:
            continue
            
        # [ê²€ì¦ C] ì œëª© ì• 15ê¸€ìë¡œ ì¤‘ë³µ(ë„ë°°) ì—¬ë¶€ í™•ì¸
        is_title_duplicate = False
        prefix = title[:15]
        for existing in current_titles:
            if prefix in existing:
                is_title_duplicate = True
                break
        
        if not is_title_duplicate:
            current_titles.append(title)
            
            # [ìš°ì„ ìˆœìœ„ ì„¤ì •] ì œëª©ì— í‚¤ì›Œë“œê°€ ìˆìœ¼ë©´ 0ìˆœìœ„, ì—†ìœ¼ë©´ 1ìˆœìœ„ë¡œ ê¸°ë¡í•©ë‹ˆë‹¤.
            has_kw = 0 if kw.lower() in title.lower() else 1
            
            all_collected_articles.append({
                "kw": kw,
                "title": title,
                "link": link,
                "raw_date": pub_dt, # ì •ë ¬ìš© ì‹¤ì œ ë‚ ì§œ ê°ì²´
                "kor_date": format_date_kor(pub_date_raw), # í™”ë©´ í‘œì‹œìš© í•œêµ­ì–´ ë‚ ì§œ
                "priority": has_kw # ì •ë ¬ ìš°ì„ ìˆœìœ„ ê°’
            })
            sent_links.add(link)

# 5. [ì •ë ¬] ì œëª©ì— í‚¤ì›Œë“œê°€ í¬í•¨ëœ ê²ƒì´ ìœ„ë¡œ ì˜¤ê²Œ í•˜ê³ , ê·¸ë‹¤ìŒ ìµœì‹ ìˆœìœ¼ë¡œ ì •ë ¬í•©ë‹ˆë‹¤.
all_collected_articles.sort(key=lambda x: (x['priority'], x['raw_date']), reverse=True)
# priorityëŠ” 0ì´ 1ë³´ë‹¤ ì•ì— ì™€ì•¼ í•˜ë¯€ë¡œ í•œ ë²ˆ ë” ì¡ì•„ì¤ë‹ˆë‹¤.
all_collected_articles.sort(key=lambda x: x['priority'])

# 6. ê²°ê³¼ ì „ì†¡ ë° ê¸°ë¡ ì €ì¥
if all_collected_articles:
    formatted_msgs = []
    for art in all_collected_articles:
        formatted_msgs.append(f"â€¢ <b>[{art['kw']}]</b> {art['title']}\n  ğŸ•’ {art['kor_date']}\n  <a href='{art['link']}'>ê¸°ì‚¬ë³´ê¸°</a>")

    # (ìˆ˜ì •ì‚¬í•­) ê¸°ì‚¬ 20ê°œì”© ë¬¶ì–´ì„œ í•˜ë‚˜ì˜ ë©”ì‹œì§€ë¡œ ë°œì†¡í•©ë‹ˆë‹¤.
    chunk_size = 20 
    for i in range(0, len(formatted_msgs), chunk_size):
        chunk = formatted_list[i:i + chunk_size] if (formatted_list := formatted_msgs) else []
        send_tg("<b>[ì„ ë³„ ë‰´ìŠ¤ ë¦¬í¬íŠ¸]</b>\n\n" + "\n\n".join(chunk))

    # (ìˆ˜ì •ì‚¬í•­) ë°œì†¡ ê¸°ë¡ì„ ìµœì‹  450ê°œê¹Œì§€ íŒŒì¼ì— ì €ì¥í•©ë‹ˆë‹¤.
    with open(DB_FILE, "w") as f:
        f.write("\n".join(list(sent_links)[-450:]))
    print(f"ì„±ê³µ: {len(all_collected_articles)}ê±´ì˜ ê¸°ì‚¬ë¥¼ ë°œì†¡í•˜ê³  450ê°œ ê¸°ë¡ì„ ì €ì¥í–ˆìŠµë‹ˆë‹¤.")
else:
    print("ìƒˆë¡œìš´ ê¸°ì‚¬ê°€ ì—†ì–´ ì¢…ë£Œí•©ë‹ˆë‹¤.")
